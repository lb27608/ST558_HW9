---
title: "ST 558 Homework 8/9"
author: "Lee Bennett"
format: html
editor: visual
---

```{r}
#| include: false

library(readr)
library(tidyverse)
library(purrr)
library(tidymodels)
library(glmnet)
library(tree)
library(baguette)
library(ranger)
library(vip)
```

## Introduction

## Importing the Data

The first step is to read in the data via its URL. Using the option `locale` option eliminates the `invalid multibyte string, element 1` error:

```{r}
bikeData <- read_csv(file="https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv",locale=locale(encoding="latin1"))
```

## Exploratory Data Analysis

As the first step of the data exploration, we'll check for missing values and compute some summaries for both the numeric and categorical variables in the dataset:

```{r missing data check and summary stats}
#Check for NA values
colSums(is.na(bikeData))

#Compute summary stats for all numeric variables, rounding to 2 decimal places
pivot_longer(bikeData |> summarize(across(where(is.numeric),list("mean"=mean,"sd"=sd),.names="{.fn}_{.col}")),everything()) |> mutate (across(where(is.numeric), ~num(.x,digits=2)))

#Summarize levels for categorical variables
levels(as_factor(bikeData$Holiday))
levels(as_factor(bikeData$`Functioning Day`))
levels(as_factor(bikeData$Seasons))
```

There is no missing data to consider. The summary statistics for the bike counts and the weather variables appear to be reasonable, and the levels of the categorical variables are what we would expect.

The `Date` variable is a character vector in "dd/mm//yyyy" format, we'll use `lubridate` to convert it to a proper date format. We also convert the categorical (character) variables into factors and rename the variables to a standard format for further analyses:

```{r}
#| warning: false

bikeData_rev <- bikeData |> mutate(Date=dmy(Date),
                               Seasons=as.factor(Seasons),
                               Holiday=as.factor(Holiday),
                               `Functioning Day`=as.factor(`Functioning Day`))

#Create a vector of new column names to apply to the tibble

new_names <- c("date","rented_bike_count","hour","temperature","humidity","wind_speed","visibility",
               "dew_point","solar_radiation","rainfall","snowfall","seasons","holiday",
               "functioning_day")

names(bikeData_rev) <- new_names

```

Next, let's look at a summary of bike rental days across the categorical variables for season, holiday, and functioning day:

```{r}
bikeData_rev |> group_by(seasons) |> summarize(n=sum(rented_bike_count))
bikeData_rev |> group_by(holiday) |> summarize(n=sum(rented_bike_count))
bikeData_rev |> group_by(functioning_day) |> summarize(n=sum(rented_bike_count))
```

We see that no bikes are rented on non-functioning days, so we can subset the data to include only functioning days. To create the dataset for modeling, we'll summarize the hourly data for the number of bikes rented, rainfall, and snowfall to create one observation per day:

```{r}
bike_daily <- bikeData_rev |> filter(functioning_day == "Yes") |> group_by(date,seasons,holiday) |> summarize("rainfall"=sum(rainfall),"snowfall"=sum(snowfall),"bikes_rented"=sum(rented_bike_count),                           "mean_temp"=mean(temperature),"mean_humidity"=mean(humidity),"mean_visibility"=mean(visibility), "mean_solar_radiation"=mean(solar_radiation),
              "mean_wind"=mean(wind_speed), "mean_dew_point"=mean(dew_point)) |> select(date,seasons,holiday,rainfall,snowfall,bikes_rented,starts_with("mean"))


```

Let's explore the summarized data by first looking at the same summary statistics for the number of bikes rented and then creating some scatterplots to see how the number of rentals varies by rainfall and mean temperature:

```{r}
#Summarize by seasons and holiday
bike_daily |> group_by(seasons) |> summarize(n=sum(bikes_rented))
bike_daily |> group_by(holiday) |> summarize(n=sum(bikes_rented))

rain_plot <- ggplot(bike_daily, aes(x = rainfall, y = bikes_rented)) + geom_point(position="jitter") + labs(x="Mean rainfall", y="Number of bikes rented", title="Bike Rentals vs. Rainfall") + theme(plot.title = element_text(hjust = 0.5))
rain_plot

temp_plot <- ggplot(bike_daily, aes(x = mean_temp, y = bikes_rented)) + geom_point(position="jitter") + labs(x="Mean temperature", y="Number of bikes rented", title="Bike Rentals vs. Mean Temperature") + theme(plot.title = element_text(hjust = 0.5))
temp_plot
```
The plot of bike rentals vs. mean rainfall is heavily skewed, with most of the observations clustered at 0 (no rainfall). The scatterplot for mean temperature suggests a nonlinear relationship with the number of bikes rented; that value peaks around 22 degrees and then tends to decrease. This association suggests that a higher-order term (e.g., quadratic) may be needed in a model to predict bike rentals using temperature.

## Partitioning the Data

We'll now partition the data, stratifying by season, into a training set (75%) and a test set (25%) and then create a 10-fold cross-validation set using the training data. 

```{r}

#Create initial split
set.seed(1434)
bike_split <- initial_split(bike_daily, prop=0.75, strata=seasons)
bike_train <- training(bike_split)
bike_test <- testing(bike_split)

#Create 10-fold cross-validation sets on the training data
bike_10_fold <- vfold_cv(bike_train,10)
```

## Model Training

We'll consider three models for predicting the total number of bike rentals per day. The first will have only main effects for type of day (weekday or weekend), seasons, and holiday along with predictors for the average weather variables for each day:
```{r recipe 1 - base model}
recipe1 <- recipe(bikes_rented ~ ., data = bike_train) |>
  step_date(date,features="dow") |>
  step_mutate(day_type=factor(if_else(date_dow %in% c("Sat","Sun"),"Weekend","Weekday"))) |>
  step_rm(date,date_dow) |>
  step_dummy(seasons,holiday,day_type) |>
  step_normalize(all_numeric(), -all_outcomes())

prep(recipe1)
```

The second model extends the first model by adding interaction terms for season with holiday status and mean temperature along with an interaction between rainfall and mean temperature:

```{r recipe 2 - interactions}
recipe2 <- recipe(bikes_rented ~ ., data = bike_train) |>
  step_date(date,features="dow") |>
  step_mutate(day_type=factor(if_else(date_dow %in% c("Sat","Sun"),"Weekend","Weekday"))) |>
  step_rm(date,date_dow) |>
  step_dummy(seasons,holiday,day_type) |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_interact(terms = ~ starts_with("seasons"):holiday_No.Holiday + starts_with("seasons"):mean_temp + rainfall:mean_temp)

prep(recipe2)
```

The last model adds to the second model by including quadratic terms for all of the continuous weather-related variables using orthogonal polynomials using `step_poly`:

```{r recipe 3 - quadratic terms}
recipe3 <- recipe(bikes_rented ~ ., data = bike_train) |>
  step_date(date,features="dow") |>
  step_mutate(day_type=factor(if_else(date_dow %in% c("Sat","Sun"),"Weekend","Weekday"))) |>
  step_rm(date,date_dow) |>
  step_dummy(seasons,holiday,day_type) |>
  step_normalize(all_numeric(), -all_outcomes()) |>
  step_interact(terms = ~ starts_with("seasons"):holiday_No.Holiday + starts_with("seasons"):mean_temp + rainfall:mean_temp) |>
  step_poly(rainfall, snowfall, mean_temp, mean_humidity, mean_visibility, mean_solar_radiation, mean_wind, mean_dew_point,
degree = 2)

prep(recipe3)
```

With these model recipes complete, we can train each model using the 10-fold CV training set. We'll then compare the model fits using the RMSE and R-squared metrics:

```{r train models}
#Set model and model engine
bike_model <- linear_reg() |> set_engine("lm")

#Create workflows
bike_wf1 <- workflow() |> add_recipe(recipe1) |> add_model(bike_model)
bike_wf1

bike_wf2 <- workflow() |> add_recipe(recipe2) |> add_model(bike_model)
bike_wf2

bike_wf3 <- workflow() |> add_recipe(recipe3) |> add_model(bike_model)
bike_wf3

#Fit the models and summarize model fit metrics
bike_fit1 <- bike_wf1 |> fit_resamples(bike_10_fold)
bike_fit2 <- bike_wf2 |> fit_resamples(bike_10_fold)
bike_fit3 <- bike_wf3 |> fit_resamples(bike_10_fold)

fit_metrics <- rbind(bike_fit1 |> collect_metrics(),bike_fit2 |> collect_metrics(),bike_fit3 |> collect_metrics())
fit_metrics

```

## Final Predictive Model

Based on the fit metrics, the third model has the smallest RMSE and the largest R-squared, so it appears to be the best fit for the data among the 3 models that were tested. We can now fit this model to the entire training data set created with `initial_split` and obtain the final fit metrics:

```{r}
final_fit <- bike_wf3 |> last_fit(bike_split)
final_metrics <- final_fit |> collect_metrics()
final_metrics
```

## Homework 9: Additional Models

Next, we'll add to the linear regression models by fitting LASSO models, regression tree models, bagged tree models, and random forest models. Like the linear regression models, the best model in each class will be selected using the 10-fold CV training set. The selected model will then be fit using the full training set and then tested on the test split.

### LASSO model

```{r LASSO model}
LASSO_spec <- linear_reg(penalty = tune(), mixture = 1) |> set_engine("glmnet")

#Use the same recipe as the third linear regression model above
LASSO_recipe <- recipe3

#Create the workflow
LASSO_wkf <- workflow() |> add_recipe(LASSO_recipe) |> add_model(LASSO_spec)
LASSO_wkf

#Specify the grid for the tuning parameter
LASSO_grid <- LASSO_wkf |> tune_grid(resamples = bike_10_fold,
              grid = grid_regular(penalty(), levels = 200)) 

#Collect metrics and choose model with lowest RMSE
LASSO_grid |> collect_metrics() |> filter(.metric == "rmse")

lowest_rmse <- LASSO_grid |> select_best(metric = "rmse")

#Finalize the workflow and fit the model on the full training set
LASSO_final_wkf <- LASSO_wkf |> finalize_workflow(lowest_rmse)
```

### Regression tree model

Next, we'll fit a regression tree model using the `rpart` engine:

```{r Regression tree model}
tree_spec <- decision_tree(tree_depth = tune(),
                          min_n = 20,
                          cost_complexity = tune()) |>
  set_engine("rpart") |>
  set_mode("regression")

#Use the first regression model recipe since interactions aren't needed
tree_recipe <- recipe1

#Create the workflow
tree_wkf <- workflow() |> add_recipe(tree_recipe) |> add_model(tree_spec)


tree_grid <- tree_wkf |> tune_grid(resamples = bike_10_fold)

#Collect metrics and choose model with lowest RMSE
tree_grid |> collect_metrics() |> filter(.metric == "rmse")

tree_best_params <- select_best(tree_grid, metric="rmse")
tree_best_params

#Finalize the workflow and run the final model on the full training set
tree_final_wkf <- tree_wkf |> finalize_workflow(tree_best_params)

```

## Bagged Tree Model

For the first ensemble model, we'll fit a bagged tree:

```{r Bagged tree model}
bag_spec <- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |>
set_engine("rpart") |> set_mode("regression")

#Use the first regression model recipe again
bag_recipe <- recipe1

#Create the workflow
bag_wkf <- workflow() |> add_recipe(bag_recipe) |> add_model(bag_spec)

bag_fit <- bag_wkf |> tune_grid(resamples = bike_10_fold,
                                grid = grid_regular(cost_complexity(),
                                                    levels = 15))

bag_fit |> collect_metrics() |> filter(.metric == "rmse")

bag_best_params <- select_best(bag_fit, metric="rmse")
bag_best_params

#Finalize the workflow and run the final model on the full training set
bag_final_wkf <- bag_wkf |> finalize_workflow(bag_best_params)

```

## Random forest model

Finally, our second ensemble model will be a random forest:

```{r Random forest model}
rf_spec <- rand_forest(mtry = tune()) |> set_engine("ranger",importance="impurity") |>
  set_mode("regression")

#Use the first regression model recipe again
rf_recipe <- recipe1

#Create the workflow
rf_wkf <- workflow() |> add_recipe(rf_recipe) |> add_model(rf_spec)

rf_fit <- rf_wkf |> tune_grid(resamples = bike_10_fold,
                              grid = 7)

rf_fit |> collect_metrics() |> filter(.metric == "rmse")

rf_best_params <- select_best(rf_fit, metric="rmse")

#Finalize the workflow and run the final model on the full training set
rf_final_wkf <- rf_wkf |> finalize_workflow(rf_best_params)
```

## Compare models on test set

Let's compare all of the model predictions on the test set using RMSE and MAE as the metrics:

```{r}
#Get predictions from final models on test dataset
MLR_final_pred <- bike_wf3 |> fit(bike_train) |> predict(bike_test) |> pull()
LASSO_final_pred <- LASSO_final_wkf |> fit(bike_train) |> predict(bike_test) |> pull()
tree_final_pred <- tree_final_wkf |> fit(bike_train) |> predict(bike_test) |> pull()
bag_final_pred <- bag_final_wkf |> fit(bike_train) |> predict(bike_test) |> pull()
rf_final_pred <- rf_final_wkf |> fit(bike_train) |> predict(bike_test) |> pull()

true_rentals <- bike_test$bikes_rented

#Compute RMSE and MAE for each model
MLR_metrics <- c("rmse"=MLR_final_pred |> rmse_vec(truth = true_rentals),"mae"=MLR_final_pred |> mae_vec(truth = true_rentals))
MLR_metrics

LASSO_metrics <- c("rmse"=LASSO_final_pred |> rmse_vec(truth = true_rentals),"mae"=LASSO_final_pred |> mae_vec(truth = true_rentals))
LASSO_metrics

tree_metrics <- c("rmse"=tree_final_pred |> rmse_vec(truth = true_rentals),"mae"=tree_final_pred |> mae_vec(truth = true_rentals))
tree_metrics

bag_metrics <- c("rmse"=bag_final_pred |> rmse_vec(truth = true_rentals),"mae"=bag_final_pred |> mae_vec(truth = true_rentals))
bag_metrics

rf_metrics <- c("rmse"=rf_final_pred |> rmse_vec(truth = true_rentals),"mae"=rf_final_pred |> mae_vec(truth = true_rentals))
rf_metrics
```

The random forest model has the smallest RMSE, but the multiple linear regression has the smallest MAE among these classes of models. Let's summarize the structure of each of these models, starting with model effects tables for the multiple linear regression and LASSO models:

```{r}
MLR_model <- bike_wf3 |> fit(bike_train)
tidy(MLR_model)

LASSO_model <- LASSO_final_wkf |> fit(bike_train) 
tidy(LASSO_model)
```
For the regression tree model, we'll look at a tree plot:

```{r}
tree_final <- tree_final_wkf |> fit(bike_train) |> extract_fit_engine()
plot(tree_final)
text(tree_final,cex=0.6)
```
That's sort of ugly, but I can't figure out how to format the text labels properly!

Finally, for the bagged tree model and random forest model, we'll look at variable importance plots:

```{r}
bag_final_model<- bag_final_wkf |> fit(bike_train) |> extract_fit_engine()
bag_final_model$imp |>
mutate(term = factor(term, levels = term)) |>
ggplot(aes(x = term, y = value)) + labs(title="Variable Imporance for Bagged Model") +
theme(plot.title = element_text(hjust = 0.5)) +
geom_bar(stat ="identity") +
coord_flip()

rf_res <- last_fit(rf_final_wkf, split = bike_split)
extract_fit_parsnip(rf_res$.workflow[[1]]) |> vip::vi() |> mutate(term=factor(Variable, levels=Variable)) |>
ggplot(aes(x = term, y = Importance)) + labs(title="Variable Imporance for Random Forest Model") +
theme(plot.title = element_text(hjust = 0.5)) +
geom_bar(stat ="identity") +
coord_flip()
```

For both of these ensemble models, we can see that the average daily temperature is the most important predictor, with solar radiation and dew point being second and third most important.

If we use RMSE as the criterion for choosing the best model, then the random forest would be selected. We'll now fit that model on the full dataset:

```{r Random forest full dataset}
#Fit model to the full dataset and obtain predictions
rf_full_pred<- rf_final_wkf |> fit(bike_daily) |> predict(bike_daily) |> pull()
true_bikes <- bike_daily$bikes_rented

#Get RMSE and MAE for predictions
rf_metrics <- c("rmse"=rf_full_pred |> rmse_vec(truth = true_bikes),"mae"=rf_full_pred |> mae_vec(truth = true_bikes))
rf_metrics

```

