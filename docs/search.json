[
  {
    "objectID": "ST558_HW9.html#importing-the-data",
    "href": "ST558_HW9.html#importing-the-data",
    "title": "ST 558 Homework 8/9",
    "section": "Importing the Data",
    "text": "Importing the Data\nThe first step is to read in the data via its URL. Using the option locale option eliminates the invalid multibyte string, element 1 error:\n\nbikeData &lt;- read_csv(file=\"https://www4.stat.ncsu.edu/~online/datasets/SeoulBikeData.csv\",locale=locale(encoding=\"latin1\"))\n\nRows: 8760 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Date, Seasons, Holiday, Functioning Day\ndbl (10): Rented Bike Count, Hour, Temperature(°C), Humidity(%), Wind speed ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "ST558_HW9.html#exploratory-data-analysis",
    "href": "ST558_HW9.html#exploratory-data-analysis",
    "title": "ST 558 Homework 8/9",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nAs the first step of the data exploration, we’ll check for missing values and compute some summaries for both the numeric and categorical variables in the dataset:\n\n#Check for NA values\ncolSums(is.na(bikeData))\n\n                     Date         Rented Bike Count                      Hour \n                        0                         0                         0 \n          Temperature(°C)               Humidity(%)          Wind speed (m/s) \n                        0                         0                         0 \n         Visibility (10m) Dew point temperature(°C)   Solar Radiation (MJ/m2) \n                        0                         0                         0 \n             Rainfall(mm)             Snowfall (cm)                   Seasons \n                        0                         0                         0 \n                  Holiday           Functioning Day \n                        0                         0 \n\n#Compute summary stats for all numeric variables, rounding to 2 decimal places\npivot_longer(bikeData |&gt; summarize(across(where(is.numeric),list(\"mean\"=mean,\"sd\"=sd),.names=\"{.fn}_{.col}\")),everything()) |&gt; mutate (across(where(is.numeric), ~num(.x,digits=2)))\n\n# A tibble: 20 × 2\n   name                               value\n   &lt;chr&gt;                          &lt;num:.2!&gt;\n 1 mean_Rented Bike Count            704.60\n 2 sd_Rented Bike Count              645.00\n 3 mean_Hour                          11.50\n 4 sd_Hour                             6.92\n 5 mean_Temperature(°C)               12.88\n 6 sd_Temperature(°C)                 11.94\n 7 mean_Humidity(%)                   58.23\n 8 sd_Humidity(%)                     20.36\n 9 mean_Wind speed (m/s)               1.72\n10 sd_Wind speed (m/s)                 1.04\n11 mean_Visibility (10m)            1436.83\n12 sd_Visibility (10m)               608.30\n13 mean_Dew point temperature(°C)      4.07\n14 sd_Dew point temperature(°C)       13.06\n15 mean_Solar Radiation (MJ/m2)        0.57\n16 sd_Solar Radiation (MJ/m2)          0.87\n17 mean_Rainfall(mm)                   0.15\n18 sd_Rainfall(mm)                     1.13\n19 mean_Snowfall (cm)                  0.08\n20 sd_Snowfall (cm)                    0.44\n\n#Summarize levels for categorical variables\nlevels(as_factor(bikeData$Holiday))\n\n[1] \"No Holiday\" \"Holiday\"   \n\nlevels(as_factor(bikeData$`Functioning Day`))\n\n[1] \"Yes\" \"No\" \n\nlevels(as_factor(bikeData$Seasons))\n\n[1] \"Winter\" \"Spring\" \"Summer\" \"Autumn\"\n\n\nThere is no missing data to consider. The summary statistics for the bike counts and the weather variables appear to be reasonable, and the levels of the categorical variables are what we would expect.\nThe Date variable is a character vector in “dd/mm//yyyy” format, we’ll use lubridate to convert it to a proper date format. We also convert the categorical (character) variables into factors and rename the variables to a standard format for further analyses:\n\nbikeData_rev &lt;- bikeData |&gt; mutate(Date=dmy(Date),\n                               Seasons=as.factor(Seasons),\n                               Holiday=as.factor(Holiday),\n                               `Functioning Day`=as.factor(`Functioning Day`))\n\n#Create a vector of new column names to apply to the tibble\n\nnew_names &lt;- c(\"date\",\"rented_bike_count\",\"hour\",\"temperature\",\"humidity\",\"wind_speed\",\"visibility\",\n               \"dew_point\",\"solar_radiation\",\"rainfall\",\"snowfall\",\"seasons\",\"holiday\",\n               \"functioning_day\")\n\nnames(bikeData_rev) &lt;- new_names\n\nNext, let’s look at a summary of bike rental days across the categorical variables for season, holiday, and functioning day:\n\nbikeData_rev |&gt; group_by(seasons) |&gt; summarize(n=sum(rented_bike_count))\n\n# A tibble: 4 × 2\n  seasons       n\n  &lt;fct&gt;     &lt;dbl&gt;\n1 Autumn  1790002\n2 Spring  1611909\n3 Summer  2283234\n4 Winter   487169\n\nbikeData_rev |&gt; group_by(holiday) |&gt; summarize(n=sum(rented_bike_count))\n\n# A tibble: 2 × 2\n  holiday          n\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holiday     215895\n2 No Holiday 5956419\n\nbikeData_rev |&gt; group_by(functioning_day) |&gt; summarize(n=sum(rented_bike_count))\n\n# A tibble: 2 × 2\n  functioning_day       n\n  &lt;fct&gt;             &lt;dbl&gt;\n1 No                    0\n2 Yes             6172314\n\n\nWe see that no bikes are rented on non-functioning days, so we can subset the data to include only functioning days. To create the dataset for modeling, we’ll summarize the hourly data for the number of bikes rented, rainfall, and snowfall to create one observation per day:\n\nbike_daily &lt;- bikeData_rev |&gt; filter(functioning_day == \"Yes\") |&gt; group_by(date,seasons,holiday) |&gt; summarize(\"rainfall\"=sum(rainfall),\"snowfall\"=sum(snowfall),\"bikes_rented\"=sum(rented_bike_count),                           \"mean_temp\"=mean(temperature),\"mean_humidity\"=mean(humidity),\"mean_visibility\"=mean(visibility), \"mean_solar_radiation\"=mean(solar_radiation),\n              \"mean_wind\"=mean(wind_speed), \"mean_dew_point\"=mean(dew_point)) |&gt; select(date,seasons,holiday,rainfall,snowfall,bikes_rented,starts_with(\"mean\"))\n\n`summarise()` has grouped output by 'date', 'seasons'. You can override using\nthe `.groups` argument.\n\n\nLet’s explore the summarized data by first looking at the same summary statistics for the number of bikes rented and then creating some scatterplots to see how the number of rentals varies by rainfall and mean temperature:\n\n#Summarize by seasons and holiday\nbike_daily |&gt; group_by(seasons) |&gt; summarize(n=sum(bikes_rented))\n\n# A tibble: 4 × 2\n  seasons       n\n  &lt;fct&gt;     &lt;dbl&gt;\n1 Autumn  1790002\n2 Spring  1611909\n3 Summer  2283234\n4 Winter   487169\n\nbike_daily |&gt; group_by(holiday) |&gt; summarize(n=sum(bikes_rented))\n\n# A tibble: 2 × 2\n  holiday          n\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Holiday     215895\n2 No Holiday 5956419\n\nrain_plot &lt;- ggplot(bike_daily, aes(x = rainfall, y = bikes_rented)) + geom_point(position=\"jitter\") + labs(x=\"Mean rainfall\", y=\"Number of bikes rented\", title=\"Bike Rentals vs. Rainfall\") + theme(plot.title = element_text(hjust = 0.5))\nrain_plot\n\n\n\n\n\n\n\ntemp_plot &lt;- ggplot(bike_daily, aes(x = mean_temp, y = bikes_rented)) + geom_point(position=\"jitter\") + labs(x=\"Mean temperature\", y=\"Number of bikes rented\", title=\"Bike Rentals vs. Mean Temperature\") + theme(plot.title = element_text(hjust = 0.5))\ntemp_plot\n\n\n\n\n\n\n\n\nThe plot of bike rentals vs. mean rainfall is heavily skewed, with most of the observations clustered at 0 (no rainfall). The scatterplot for mean temperature suggests a nonlinear relationship with the number of bikes rented; that value peaks around 22 degrees and then tends to decrease. This association suggests that a higher-order term (e.g., quadratic) may be needed in a model to predict bike rentals using temperature."
  },
  {
    "objectID": "ST558_HW9.html#partitioning-the-data",
    "href": "ST558_HW9.html#partitioning-the-data",
    "title": "ST 558 Homework 8/9",
    "section": "Partitioning the Data",
    "text": "Partitioning the Data\nWe’ll now partition the data, stratifying by season, into a training set (75%) and a test set (25%) and then create a 10-fold cross-validation set using the training data.\n\n#Create initial split\nset.seed(1434)\nbike_split &lt;- initial_split(bike_daily, prop=0.75, strata=seasons)\nbike_train &lt;- training(bike_split)\nbike_test &lt;- testing(bike_split)\n\n#Create 10-fold cross-validation sets on the training data\nbike_10_fold &lt;- vfold_cv(bike_train,10)"
  },
  {
    "objectID": "ST558_HW9.html#model-training",
    "href": "ST558_HW9.html#model-training",
    "title": "ST 558 Homework 8/9",
    "section": "Model Training",
    "text": "Model Training\nWe’ll consider three models for predicting the total number of bike rentals per day. The first will have only main effects for type of day (weekday or weekend), seasons, and holiday along with predictors for the average weather variables for each day:\n\nrecipe1 &lt;- recipe(bikes_rented ~ ., data = bike_train) |&gt;\n  step_date(date,features=\"dow\") |&gt;\n  step_mutate(day_type=factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\"Weekend\",\"Weekday\"))) |&gt;\n  step_rm(date,date_dow) |&gt;\n  step_dummy(seasons,holiday,day_type) |&gt;\n  step_normalize(all_numeric(), -all_outcomes())\n\nprep(recipe1)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Training information \n\n\nTraining data contained 263 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Date features from: date | Trained\n\n\n• Variable mutation for: ~factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\")) | Trained\n\n\n• Variables removed: date and date_dow | Trained\n\n\n• Dummy variables from: seasons, holiday, day_type | Trained\n\n\n• Centering and scaling for: rainfall, snowfall, mean_temp, ... | Trained\n\n\nThe second model extends the first model by adding interaction terms for season with holiday status and mean temperature along with an interaction between rainfall and mean temperature:\n\nrecipe2 &lt;- recipe(bikes_rented ~ ., data = bike_train) |&gt;\n  step_date(date,features=\"dow\") |&gt;\n  step_mutate(day_type=factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\"Weekend\",\"Weekday\"))) |&gt;\n  step_rm(date,date_dow) |&gt;\n  step_dummy(seasons,holiday,day_type) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_interact(terms = ~ starts_with(\"seasons\"):holiday_No.Holiday + starts_with(\"seasons\"):mean_temp + rainfall:mean_temp)\n\nprep(recipe2)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Training information \n\n\nTraining data contained 263 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Date features from: date | Trained\n\n\n• Variable mutation for: ~factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\")) | Trained\n\n\n• Variables removed: date and date_dow | Trained\n\n\n• Dummy variables from: seasons, holiday, day_type | Trained\n\n\n• Centering and scaling for: rainfall, snowfall, mean_temp, ... | Trained\n\n\n• Interactions with: (seasons_Spring + seasons_Summer +\n  seasons_Winter):holiday_No.Holiday + (seasons_Spring + seasons_Summer +\n  seasons_Winter):mean_temp + rainfall:mean_temp | Trained\n\n\nThe last model adds to the second model by including quadratic terms for all of the continuous weather-related variables using orthogonal polynomials using step_poly:\n\nrecipe3 &lt;- recipe(bikes_rented ~ ., data = bike_train) |&gt;\n  step_date(date,features=\"dow\") |&gt;\n  step_mutate(day_type=factor(if_else(date_dow %in% c(\"Sat\",\"Sun\"),\"Weekend\",\"Weekday\"))) |&gt;\n  step_rm(date,date_dow) |&gt;\n  step_dummy(seasons,holiday,day_type) |&gt;\n  step_normalize(all_numeric(), -all_outcomes()) |&gt;\n  step_interact(terms = ~ starts_with(\"seasons\"):holiday_No.Holiday + starts_with(\"seasons\"):mean_temp + rainfall:mean_temp) |&gt;\n  step_poly(rainfall, snowfall, mean_temp, mean_humidity, mean_visibility, mean_solar_radiation, mean_wind, mean_dew_point,\ndegree = 2)\n\nprep(recipe3)\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:    1\npredictor: 11\n\n\n\n\n\n── Training information \n\n\nTraining data contained 263 data points and no incomplete rows.\n\n\n\n\n\n── Operations \n\n\n• Date features from: date | Trained\n\n\n• Variable mutation for: ~factor(if_else(date_dow %in% c(\"Sat\", \"Sun\"),\n  \"Weekend\", \"Weekday\")) | Trained\n\n\n• Variables removed: date and date_dow | Trained\n\n\n• Dummy variables from: seasons, holiday, day_type | Trained\n\n\n• Centering and scaling for: rainfall, snowfall, mean_temp, ... | Trained\n\n\n• Interactions with: (seasons_Spring + seasons_Summer +\n  seasons_Winter):holiday_No.Holiday + (seasons_Spring + seasons_Summer +\n  seasons_Winter):mean_temp + rainfall:mean_temp | Trained\n\n\n• Orthogonal polynomials on: rainfall, snowfall, mean_temp, ... | Trained\n\n\nWith these model recipes complete, we can train each model using the 10-fold CV training set. We’ll then compare the model fits using the RMSE and R-squared metrics:\n\n#Set model and model engine\nbike_model &lt;- linear_reg() |&gt; set_engine(\"lm\")\n\n#Create workflows\nbike_wf1 &lt;- workflow() |&gt; add_recipe(recipe1) |&gt; add_model(bike_model)\nbike_wf1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nbike_wf2 &lt;- workflow() |&gt; add_recipe(recipe2) |&gt; add_model(bike_model)\nbike_wf2\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n• step_interact()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\nbike_wf3 &lt;- workflow() |&gt; add_recipe(recipe3) |&gt; add_model(bike_model)\nbike_wf3\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n• step_interact()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n#Fit the models and summarize model fit metrics\nbike_fit1 &lt;- bike_wf1 |&gt; fit_resamples(bike_10_fold)\nbike_fit2 &lt;- bike_wf2 |&gt; fit_resamples(bike_10_fold)\nbike_fit3 &lt;- bike_wf3 |&gt; fit_resamples(bike_10_fold)\n\nfit_metrics &lt;- rbind(bike_fit1 |&gt; collect_metrics(),bike_fit2 |&gt; collect_metrics(),bike_fit3 |&gt; collect_metrics())\nfit_metrics\n\n# A tibble: 6 × 6\n  .metric .estimator     mean     n  std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;         &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   4141.       10 208.     Preprocessor1_Model1\n2 rsq     standard      0.822    10   0.0288 Preprocessor1_Model1\n3 rmse    standard   3002.       10 140.     Preprocessor1_Model1\n4 rsq     standard      0.908    10   0.0137 Preprocessor1_Model1\n5 rmse    standard   2993.       10 158.     Preprocessor1_Model1\n6 rsq     standard      0.911    10   0.0127 Preprocessor1_Model1"
  },
  {
    "objectID": "ST558_HW9.html#final-predictive-model",
    "href": "ST558_HW9.html#final-predictive-model",
    "title": "ST 558 Homework 8/9",
    "section": "Final Predictive Model",
    "text": "Final Predictive Model\nBased on the fit metrics, the third model has the smallest RMSE and the largest R-squared, so it appears to be the best fit for the data among the 3 models that were tested. We can now fit this model to the entire training data set created with initial_split and obtain the final fit metrics:\n\nfinal_fit &lt;- bike_wf3 |&gt; last_fit(bike_split)\nfinal_metrics &lt;- final_fit |&gt; collect_metrics()\nfinal_metrics\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard    3042.    Preprocessor1_Model1\n2 rsq     standard       0.902 Preprocessor1_Model1"
  },
  {
    "objectID": "ST558_HW9.html#homework-9-additional-models",
    "href": "ST558_HW9.html#homework-9-additional-models",
    "title": "ST 558 Homework 8/9",
    "section": "Homework 9: Additional Models",
    "text": "Homework 9: Additional Models\nNext, we’ll add to the linear regression models by fitting LASSO models, regression tree models, bagged tree models, and random forest models. Like the linear regression models, the best model in each class will be selected using the 10-fold CV training set. The selected model will then be fit using the full training set and then tested on the test split.\n\nLASSO model\n\nLASSO_spec &lt;- linear_reg(penalty = tune(), mixture = 1) |&gt; set_engine(\"glmnet\")\n\n#Use the same recipe as the third linear regression model above\nLASSO_recipe &lt;- recipe3\n\n#Create the workflow\nLASSO_wkf &lt;- workflow() |&gt; add_recipe(LASSO_recipe) |&gt; add_model(LASSO_spec)\nLASSO_wkf\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n7 Recipe Steps\n\n• step_date()\n• step_mutate()\n• step_rm()\n• step_dummy()\n• step_normalize()\n• step_interact()\n• step_poly()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nMain Arguments:\n  penalty = tune()\n  mixture = 1\n\nComputational engine: glmnet \n\n#Specify the grid for the tuning parameter\nLASSO_grid &lt;- LASSO_wkf |&gt; tune_grid(resamples = bike_10_fold,\n              grid = grid_regular(penalty(), levels = 200)) \n\n#Collect metrics and choose model with lowest RMSE\nLASSO_grid |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\")\n\n# A tibble: 200 × 7\n    penalty .metric .estimator  mean     n std_err .config               \n      &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 \n 1 1   e-10 rmse    standard   2952.    10    156. Preprocessor1_Model001\n 2 1.12e-10 rmse    standard   2952.    10    156. Preprocessor1_Model002\n 3 1.26e-10 rmse    standard   2952.    10    156. Preprocessor1_Model003\n 4 1.41e-10 rmse    standard   2952.    10    156. Preprocessor1_Model004\n 5 1.59e-10 rmse    standard   2952.    10    156. Preprocessor1_Model005\n 6 1.78e-10 rmse    standard   2952.    10    156. Preprocessor1_Model006\n 7 2.00e-10 rmse    standard   2952.    10    156. Preprocessor1_Model007\n 8 2.25e-10 rmse    standard   2952.    10    156. Preprocessor1_Model008\n 9 2.52e-10 rmse    standard   2952.    10    156. Preprocessor1_Model009\n10 2.83e-10 rmse    standard   2952.    10    156. Preprocessor1_Model010\n# ℹ 190 more rows\n\nlowest_rmse &lt;- LASSO_grid |&gt; select_best(metric = \"rmse\")\n\n#Finalize the workflow and fit the model on the full training set\nLASSO_final_wkf &lt;- LASSO_wkf |&gt; finalize_workflow(lowest_rmse)\n\n\n\nRegression tree model\nNext, we’ll fit a regression tree model using the rpart engine:\n\ntree_spec &lt;- decision_tree(tree_depth = tune(),\n                          min_n = 20,\n                          cost_complexity = tune()) |&gt;\n  set_engine(\"rpart\") |&gt;\n  set_mode(\"regression\")\n\n#Use the first regression model recipe since interactions aren't needed\ntree_recipe &lt;- recipe1\n\n#Create the workflow\ntree_wkf &lt;- workflow() |&gt; add_recipe(tree_recipe) |&gt; add_model(tree_spec)\n\n\ntree_grid &lt;- tree_wkf |&gt; tune_grid(resamples = bike_10_fold)\n\n#Collect metrics and choose model with lowest RMSE\ntree_grid |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\")\n\n# A tibble: 10 × 8\n   cost_complexity tree_depth .metric .estimator  mean     n std_err .config    \n             &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;      \n 1        1.35e- 4          8 rmse    standard   3924.    10    141. Preprocess…\n 2        5.17e- 2          4 rmse    standard   5069.    10    180. Preprocess…\n 3        5.13e- 3          7 rmse    standard   3917.    10    148. Preprocess…\n 4        1.67e- 7          2 rmse    standard   4933.    10    154. Preprocess…\n 5        4.03e- 9          4 rmse    standard   4417.    10    142. Preprocess…\n 6        5.04e- 7         14 rmse    standard   3932.    10    140. Preprocess…\n 7        2.16e-10         11 rmse    standard   3932.    10    140. Preprocess…\n 8        2.53e- 8         12 rmse    standard   3932.    10    140. Preprocess…\n 9        3.56e- 6          6 rmse    standard   3894.    10    148. Preprocess…\n10        6.45e- 4         13 rmse    standard   3934.    10    143. Preprocess…\n\ntree_best_params &lt;- select_best(tree_grid, metric=\"rmse\")\ntree_best_params\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;                \n1      0.00000356          6 Preprocessor1_Model09\n\n#Finalize the workflow and run the final model on the full training set\ntree_final_wkf &lt;- tree_wkf |&gt; finalize_workflow(tree_best_params)"
  },
  {
    "objectID": "ST558_HW9.html#bagged-tree-model",
    "href": "ST558_HW9.html#bagged-tree-model",
    "title": "ST 558 Homework 8/9",
    "section": "Bagged Tree Model",
    "text": "Bagged Tree Model\nFor the first ensemble model, we’ll fit a bagged tree:\n\nbag_spec &lt;- bag_tree(tree_depth = 5, min_n = 10, cost_complexity = tune()) |&gt;\nset_engine(\"rpart\") |&gt; set_mode(\"regression\")\n\n#Use the first regression model recipe again\nbag_recipe &lt;- recipe1\n\n#Create the workflow\nbag_wkf &lt;- workflow() |&gt; add_recipe(bag_recipe) |&gt; add_model(bag_spec)\n\nbag_fit &lt;- bag_wkf |&gt; tune_grid(resamples = bike_10_fold,\n                                grid = grid_regular(cost_complexity(),\n                                                    levels = 15))\n\nbag_fit |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\")\n\n# A tibble: 15 × 7\n   cost_complexity .metric .estimator  mean     n std_err .config              \n             &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1        1   e-10 rmse    standard   3283.    10   137.  Preprocessor1_Model01\n 2        4.39e-10 rmse    standard   3300.    10    91.9 Preprocessor1_Model02\n 3        1.93e- 9 rmse    standard   3413.    10   122.  Preprocessor1_Model03\n 4        8.48e- 9 rmse    standard   3329.    10   157.  Preprocessor1_Model04\n 5        3.73e- 8 rmse    standard   3344.    10   117.  Preprocessor1_Model05\n 6        1.64e- 7 rmse    standard   3348.    10   135.  Preprocessor1_Model06\n 7        7.20e- 7 rmse    standard   3388.    10   106.  Preprocessor1_Model07\n 8        3.16e- 6 rmse    standard   3283.    10   118.  Preprocessor1_Model08\n 9        1.39e- 5 rmse    standard   3288.    10    89.7 Preprocessor1_Model09\n10        6.11e- 5 rmse    standard   3324.    10   150.  Preprocessor1_Model10\n11        2.68e- 4 rmse    standard   3288.    10   167.  Preprocessor1_Model11\n12        1.18e- 3 rmse    standard   3325.    10   130.  Preprocessor1_Model12\n13        5.18e- 3 rmse    standard   3457.    10   108.  Preprocessor1_Model13\n14        2.28e- 2 rmse    standard   3818.    10   134.  Preprocessor1_Model14\n15        1   e- 1 rmse    standard   5082.    10   205.  Preprocessor1_Model15\n\nbag_best_params &lt;- select_best(bag_fit, metric=\"rmse\")\nbag_best_params\n\n# A tibble: 1 × 2\n  cost_complexity .config              \n            &lt;dbl&gt; &lt;chr&gt;                \n1      0.00000316 Preprocessor1_Model08\n\n#Finalize the workflow and run the final model on the full training set\nbag_final_wkf &lt;- bag_wkf |&gt; finalize_workflow(bag_best_params)"
  },
  {
    "objectID": "ST558_HW9.html#random-forest-model",
    "href": "ST558_HW9.html#random-forest-model",
    "title": "ST 558 Homework 8/9",
    "section": "Random forest model",
    "text": "Random forest model\nFinally, our second ensemble model will be a random forest:\n\nrf_spec &lt;- rand_forest(mtry = tune()) |&gt; set_engine(\"ranger\",importance=\"impurity\") |&gt;\n  set_mode(\"regression\")\n\n#Use the first regression model recipe again\nrf_recipe &lt;- recipe1\n\n#Create the workflow\nrf_wkf &lt;- workflow() |&gt; add_recipe(rf_recipe) |&gt; add_model(rf_spec)\n\nrf_fit &lt;- rf_wkf |&gt; tune_grid(resamples = bike_10_fold,\n                              grid = 7)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\nrf_fit |&gt; collect_metrics() |&gt; filter(.metric == \"rmse\")\n\n# A tibble: 6 × 7\n   mtry .metric .estimator  mean     n std_err .config             \n  &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1     6 rmse    standard   3103.    10   104.  Preprocessor1_Model1\n2     9 rmse    standard   3043.    10   111.  Preprocessor1_Model2\n3    11 rmse    standard   3028.    10   112.  Preprocessor1_Model3\n4     3 rmse    standard   3274.    10    94.8 Preprocessor1_Model4\n5    12 rmse    standard   3017.    10   113.  Preprocessor1_Model5\n6     7 rmse    standard   3076.    10   104.  Preprocessor1_Model6\n\nrf_best_params &lt;- select_best(rf_fit, metric=\"rmse\")\n\n#Finalize the workflow and run the final model on the full training set\nrf_final_wkf &lt;- rf_wkf |&gt; finalize_workflow(rf_best_params)"
  },
  {
    "objectID": "ST558_HW9.html#compare-models-on-test-set",
    "href": "ST558_HW9.html#compare-models-on-test-set",
    "title": "ST 558 Homework 8/9",
    "section": "Compare models on test set",
    "text": "Compare models on test set\nLet’s compare all of the model predictions on the test set using RMSE and MAE as the metrics:\n\n#Get predictions from final models on test dataset\nMLR_final_pred &lt;- bike_wf3 |&gt; fit(bike_train) |&gt; predict(bike_test) |&gt; pull()\nLASSO_final_pred &lt;- LASSO_final_wkf |&gt; fit(bike_train) |&gt; predict(bike_test) |&gt; pull()\ntree_final_pred &lt;- tree_final_wkf |&gt; fit(bike_train) |&gt; predict(bike_test) |&gt; pull()\nbag_final_pred &lt;- bag_final_wkf |&gt; fit(bike_train) |&gt; predict(bike_test) |&gt; pull()\nrf_final_pred &lt;- rf_final_wkf |&gt; fit(bike_train) |&gt; predict(bike_test) |&gt; pull()\n\ntrue_rentals &lt;- bike_test$bikes_rented\n\n#Compute RMSE and MAE for each model\nMLR_metrics &lt;- c(\"rmse\"=MLR_final_pred |&gt; rmse_vec(truth = true_rentals),\"mae\"=MLR_final_pred |&gt; mae_vec(truth = true_rentals))\nMLR_metrics\n\n    rmse      mae \n3042.194 1968.695 \n\nLASSO_metrics &lt;- c(\"rmse\"=LASSO_final_pred |&gt; rmse_vec(truth = true_rentals),\"mae\"=LASSO_final_pred |&gt; mae_vec(truth = true_rentals))\nLASSO_metrics\n\n    rmse      mae \n3047.153 1981.590 \n\ntree_metrics &lt;- c(\"rmse\"=tree_final_pred |&gt; rmse_vec(truth = true_rentals),\"mae\"=tree_final_pred |&gt; mae_vec(truth = true_rentals))\ntree_metrics\n\n    rmse      mae \n3158.957 2426.339 \n\nbag_metrics &lt;- c(\"rmse\"=bag_final_pred |&gt; rmse_vec(truth = true_rentals),\"mae\"=bag_final_pred |&gt; mae_vec(truth = true_rentals))\nbag_metrics\n\n    rmse      mae \n3157.321 2546.378 \n\nrf_metrics &lt;- c(\"rmse\"=rf_final_pred |&gt; rmse_vec(truth = true_rentals),\"mae\"=rf_final_pred |&gt; mae_vec(truth = true_rentals))\nrf_metrics\n\n    rmse      mae \n2641.164 2092.333 \n\n\nThe random forest model has the smallest RMSE, but the multiple linear regression has the smallest MAE among these classes of models. Let’s summarize the structure of each of these models, starting with model effects tables for the multiple linear regression and LASSO models:\n\nMLR_model &lt;- bike_wf3 |&gt; fit(bike_train)\ntidy(MLR_model)\n\n# A tibble: 29 × 5\n   term                                estimate std.error statistic  p.value\n   &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)                           21977.     1564.    14.1   6.25e-33\n 2 seasons_Spring                        -1832.      248.    -7.38  2.77e-12\n 3 seasons_Summer                         7679.      942.     8.15  2.13e-14\n 4 seasons_Winter                        -4342.     1027.    -4.23  3.40e- 5\n 5 holiday_No.Holiday                      806.      187.     4.31  2.44e- 5\n 6 day_type_Weekend                      -1056.      173.    -6.11  4.06e- 9\n 7 seasons_Spring_x_holiday_No.Holiday    -317.      237.    -1.34  1.82e- 1\n 8 seasons_Summer_x_holiday_No.Holiday    -204.      252.    -0.807 4.20e- 1\n 9 seasons_Winter_x_holiday_No.Holiday    -335.      200.    -1.67  9.65e- 2\n10 seasons_Spring_x_mean_temp             1966.      482.     4.08  6.11e- 5\n# ℹ 19 more rows\n\nLASSO_model &lt;- LASSO_final_wkf |&gt; fit(bike_train) \ntidy(LASSO_model)\n\n# A tibble: 29 × 3\n   term                                estimate      penalty\n   &lt;chr&gt;                                  &lt;dbl&gt;        &lt;dbl&gt;\n 1 (Intercept)                           22731. 0.0000000001\n 2 seasons_Spring                        -1824. 0.0000000001\n 3 seasons_Summer                         7902. 0.0000000001\n 4 seasons_Winter                        -3807. 0.0000000001\n 5 holiday_No.Holiday                      799. 0.0000000001\n 6 day_type_Weekend                      -1055. 0.0000000001\n 7 seasons_Spring_x_holiday_No.Holiday    -280. 0.0000000001\n 8 seasons_Summer_x_holiday_No.Holiday    -170. 0.0000000001\n 9 seasons_Winter_x_holiday_No.Holiday    -294. 0.0000000001\n10 seasons_Spring_x_mean_temp             2104. 0.0000000001\n# ℹ 19 more rows\n\n\nFor the regression tree model, we’ll look at a tree plot:\n\ntree_final &lt;- tree_final_wkf |&gt; fit(bike_train) |&gt; extract_fit_engine()\nplot(tree_final)\ntext(tree_final,cex=0.6)\n\n\n\n\n\n\n\n\nThat’s sort of ugly, but I can’t figure out how to format the text labels properly!\nFinally, for the bagged tree model and random forest model, we’ll look at variable importance plots:\n\nbag_final_model&lt;- bag_final_wkf |&gt; fit(bike_train) |&gt; extract_fit_engine()\nbag_final_model$imp |&gt;\nmutate(term = factor(term, levels = term)) |&gt;\nggplot(aes(x = term, y = value)) + labs(title=\"Variable Imporance for Bagged Model\") +\ntheme(plot.title = element_text(hjust = 0.5)) +\ngeom_bar(stat =\"identity\") +\ncoord_flip()\n\n\n\n\n\n\n\nrf_res &lt;- last_fit(rf_final_wkf, split = bike_split)\nextract_fit_parsnip(rf_res$.workflow[[1]]) |&gt; vip::vi() |&gt; mutate(term=factor(Variable, levels=Variable)) |&gt;\nggplot(aes(x = term, y = Importance)) + labs(title=\"Variable Imporance for Random Forest Model\") +\ntheme(plot.title = element_text(hjust = 0.5)) +\ngeom_bar(stat =\"identity\") +\ncoord_flip()\n\n\n\n\n\n\n\n\nFor both of these ensemble models, we can see that the average daily temperature is the most important predictor, with solar radiation and dew point being second and third most important.\nIf we use RMSE as the criterion for choosing the best model, then the random forest would be selected. We’ll now fit that model on the full dataset:\n\n#Fit model to the full dataset and obtain predictions\nrf_full_pred&lt;- rf_final_wkf |&gt; fit(bike_daily) |&gt; predict(bike_daily) |&gt; pull()\ntrue_bikes &lt;- bike_daily$bikes_rented\n\n#Get RMSE and MAE for predictions\nrf_metrics &lt;- c(\"rmse\"=rf_full_pred |&gt; rmse_vec(truth = true_bikes),\"mae\"=rf_full_pred |&gt; mae_vec(truth = true_bikes))\nrf_metrics\n\n     rmse       mae \n1139.1010  865.4347"
  }
]